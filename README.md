# Hands on
* Basic_NN_using_JAX_.ipynb  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Q2Bo6OSkGJr5gVAhI6zjgQ-KJO8xgPhG)

   This notebook has as goal to point in a clear way how to use JAX grad function in optimization models. We ilustrate it with a simple neural network with one layer. At the end we compare the results of both found and real parameters used to simulated data.
   
* AMPs data pipeline.ipynb [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZYlnVmaA9Gs1DbSeO_r9-pKJE3Pf3_xS#scrollTo=BP4gHy15foOy)
   
   This notebooks has the goal to create an AMPs dataset pipeline using tf.data.TextLineDataset, by using this aproximation (end-to-end pipeline) we preserve randomization and facilitate AMPs transformations integrations when feeding our NN model at trainig and production time.
   
* Word2vec with Jorge Luis Borges books [![Open In Colab](https://colab.research.google.com/drive/1bQBaUdT_NQCzNMMmJ3OMjSvaAj4X9eDs?authuser=1#scrollTo=1mw1EsdKzRQZ)
   
   This notebooks applied the fundamentals of word2vec using skipgram and negative sampling using text from Jorge Luis Borges books.

# Hands on
* Basic_NN_using_JAX_.ipynb

   This notebook has as goal to point in a clear way how to use JAX grad function in optimization models. We ilustrate it with a simple neural network with one layer. At the end we compare the results of both found and real parameters used to simulated data.
   
* AMPs data pipeline.ipynb
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZYlnVmaA9Gs1DbSeO_r9-pKJE3Pf3_xS#scrollTo=BP4gHy15foOy)
   
   This notebooks has the goal to create an AMPs dataset pipeline using tf.data.TextLineDataset, it allows to feed NN models preserving randomization and integrating AMPs transformations to it. 

# Hands on
* Basic_NN_using_JAX_.ipynb

   This notebook has as goal to point in a clear way how to use JAX grad function in optimization models. We ilustrate it with a simple neural network with one layer. At the end we compare the results of both found and real parameters used to simulated data.
   
* AMPs data pipeline.ipynb

   This notebooks has the goal to create an AMPs dataset pipeline using tf.data.TextLineDataset, it allows to feed NN models preserving randomization and integrating AMPs transformations to it. 
